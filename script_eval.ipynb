{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0911ef4-d502-4c98-9536-73b94f4798cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des données...\n",
      "Captions de référence chargées: 4050 lignes\n",
      "Captions générées chargées: 810 lignes\n",
      "Nombre d'images uniques: 810\n",
      "Nombre de captions générées: 810\n",
      "✅ CIDEr et SPICE disponibles\n",
      "\n",
      "Calcul des métriques...\n",
      "Nombre d'images à évaluer: 810\n",
      "\n",
      "Calcul de CIDEr...\n",
      "✅ CIDEr: 0.4929\n",
      "\n",
      "Calcul de SPICE (peut prendre du temps)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.nustaq.serialization.FSTClazzInfo (file:/data1/rbenhsina/rbenhsina_venv/lib/python3.10/site-packages/pycocoevalcap/spice/lib/fst-2.47.jar) to field java.lang.String.value\n",
      "WARNING: Please consider reporting this to the maintainers of org.nustaq.serialization.FSTClazzInfo\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Warning: Nashorn engine is planned to be removed from a future JDK release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 2.880 s\n",
      "✅ SPICE: 0.1391\n",
      "\n",
      "Sauvegarde des résultats dans evaluations/evaluation_model_ft_vgg_dense_transformer.json...\n",
      "\n",
      "============================================================\n",
      "RÉSULTATS D'ÉVALUATION - ft_vgg_dense_transformer\n",
      "============================================================\n",
      "Nombre d'images évaluées: 810\n",
      "\n",
      "Métriques moyennes:\n",
      "------------------------------------------------------------\n",
      "BLEU-1      : 0.6145 ± 0.1829\n",
      "BLEU-2      : 0.4039 ± 0.2264\n",
      "BLEU-3      : 0.2683 ± 0.2173\n",
      "BLEU-4      : 0.1779 ± 0.1822\n",
      "METEOR      : 0.2836 ± 0.1292\n",
      "ROUGE-L     : 0.3196 ± 0.1275\n",
      "CIDEr       : 0.4929 ± 0.0000\n",
      "SPICE       : 0.1391 ± 0.0000\n",
      "F1-Score    : 0.3701 ± 0.1181\n",
      "============================================================\n",
      "\n",
      "✅ Résultats sauvegardés dans: evaluations/evaluation_model_ft_vgg_dense_transformer.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from rouge_score import rouge_scorer\n",
    "import nltk\n",
    "\n",
    "# Télécharger les ressources NLTK nécessaires (à faire une seule fois)\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "except LookupError:\n",
    "    print(\"Téléchargement des ressources NLTK...\")\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('punkt_tab')\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4')\n",
    "    print(\"✅ Ressources NLTK téléchargées!\")\n",
    "\n",
    "# ==================== CONFIGURATION ====================\n",
    "test_captions_path = \"outputs/test_captions.csv\"\n",
    "generated_captions_path = \"outputs/generated_captions/generated_captions_ft_vgg_dense_transformer.csv\"\n",
    "nom_model = \"ft_vgg_dense_transformer\"\n",
    "output_json = f\"evaluations/evaluation_model_{nom_model}.json\"\n",
    "\n",
    "# ==================== CHARGEMENT DES DONNÉES ====================\n",
    "print(\"Chargement des données...\")\n",
    "\n",
    "# Charger les captions de référence (AVEC skiprows=1 pour ignorer le header)\n",
    "test_df = pd.read_csv(test_captions_path, engine='python', sep='|', \n",
    "                      names=['image', 'caption'], skiprows=1)\n",
    "print(f\"Captions de référence chargées: {len(test_df)} lignes\")\n",
    "\n",
    "# Charger les captions générées\n",
    "gen_df = pd.read_csv(generated_captions_path, engine='python', sep='|', \n",
    "                     names=['image', 'caption'])\n",
    "print(f\"Captions générées chargées: {len(gen_df)} lignes\")\n",
    "\n",
    "# Organiser les captions de référence par image\n",
    "references = defaultdict(list)\n",
    "for _, row in test_df.iterrows():\n",
    "    img_name = row['image'].strip()\n",
    "    caption = row['caption'].strip()\n",
    "    references[img_name].append(caption)\n",
    "\n",
    "print(f\"Nombre d'images uniques: {len(references)}\")\n",
    "\n",
    "# Organiser les captions générées\n",
    "generated = {}\n",
    "for _, row in gen_df.iterrows():\n",
    "    img_name = row['image'].strip()\n",
    "    caption = row['caption'].strip()\n",
    "    generated[img_name] = caption\n",
    "\n",
    "print(f\"Nombre de captions générées: {len(generated)}\")\n",
    "\n",
    "# ==================== IMPORT PYCOCOEVALCAP ====================\n",
    "try:\n",
    "    from pycocoevalcap.cider.cider import Cider\n",
    "    from pycocoevalcap.spice.spice import Spice\n",
    "    CIDER_AVAILABLE = True\n",
    "    SPICE_AVAILABLE = True\n",
    "    print(\"✅ CIDEr et SPICE disponibles\")\n",
    "except ImportError:\n",
    "    print(\"⚠️  pycocoevalcap non installé.\")\n",
    "    CIDER_AVAILABLE = False\n",
    "    SPICE_AVAILABLE = False\n",
    "\n",
    "# ==================== FONCTIONS D'ÉVALUATION ====================\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"Tokenize le texte en mots\"\"\"\n",
    "    return nltk.word_tokenize(text.lower())\n",
    "\n",
    "def calculate_bleu_scores(references_list, hypothesis):\n",
    "    \"\"\"Calcule BLEU-1 à BLEU-4\"\"\"\n",
    "    smooth = SmoothingFunction()\n",
    "    \n",
    "    # Tokenizer\n",
    "    ref_tokens = [tokenize(ref) for ref in references_list]\n",
    "    hyp_tokens = tokenize(hypothesis)\n",
    "    \n",
    "    bleu1 = sentence_bleu(ref_tokens, hyp_tokens, weights=(1, 0, 0, 0), \n",
    "                          smoothing_function=smooth.method1)\n",
    "    bleu2 = sentence_bleu(ref_tokens, hyp_tokens, weights=(0.5, 0.5, 0, 0), \n",
    "                          smoothing_function=smooth.method1)\n",
    "    bleu3 = sentence_bleu(ref_tokens, hyp_tokens, weights=(0.33, 0.33, 0.33, 0), \n",
    "                          smoothing_function=smooth.method1)\n",
    "    bleu4 = sentence_bleu(ref_tokens, hyp_tokens, weights=(0.25, 0.25, 0.25, 0.25), \n",
    "                          smoothing_function=smooth.method1)\n",
    "    \n",
    "    return bleu1, bleu2, bleu3, bleu4\n",
    "\n",
    "def calculate_meteor(references_list, hypothesis):\n",
    "    \"\"\"Calcule METEOR score\"\"\"\n",
    "    ref_tokens = [tokenize(ref) for ref in references_list]\n",
    "    hyp_tokens = tokenize(hypothesis)\n",
    "    \n",
    "    # METEOR nécessite une seule référence à la fois, on prend la moyenne\n",
    "    scores = [meteor_score([ref], hyp_tokens) for ref in ref_tokens]\n",
    "    return np.mean(scores)\n",
    "\n",
    "def calculate_rouge_l(references_list, hypothesis):\n",
    "    \"\"\"Calcule ROUGE-L score\"\"\"\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    \n",
    "    scores = []\n",
    "    for ref in references_list:\n",
    "        score = scorer.score(ref, hypothesis)\n",
    "        scores.append(score['rougeL'].fmeasure)\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "def calculate_f1_score(references_list, hypothesis):\n",
    "    \"\"\"Calcule F1-score basé sur les mots communs\"\"\"\n",
    "    hyp_words = set(tokenize(hypothesis))\n",
    "    \n",
    "    f1_scores = []\n",
    "    for ref in references_list:\n",
    "        ref_words = set(tokenize(ref))\n",
    "        \n",
    "        if len(hyp_words) == 0 and len(ref_words) == 0:\n",
    "            f1_scores.append(1.0)\n",
    "            continue\n",
    "        \n",
    "        common = hyp_words.intersection(ref_words)\n",
    "        \n",
    "        if len(common) == 0:\n",
    "            f1_scores.append(0.0)\n",
    "            continue\n",
    "        \n",
    "        precision = len(common) / len(hyp_words) if len(hyp_words) > 0 else 0\n",
    "        recall = len(common) / len(ref_words) if len(ref_words) > 0 else 0\n",
    "        \n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    return np.mean(f1_scores)\n",
    "\n",
    "def prepare_for_coco_eval(references_dict, generated_dict, common_images):\n",
    "    \"\"\"Prépare les données au format attendu par pycocoevalcap\"\"\"\n",
    "    gts = {}\n",
    "    res = {}\n",
    "    \n",
    "    for idx, img_name in enumerate(common_images):\n",
    "        # Format: liste de strings directement\n",
    "        gts[idx] = references_dict[img_name]\n",
    "        res[idx] = [generated_dict[img_name]]\n",
    "    \n",
    "    return gts, res\n",
    "\n",
    "# ==================== CALCUL DES MÉTRIQUES ====================\n",
    "print(\"\\nCalcul des métriques...\")\n",
    "\n",
    "metrics = {\n",
    "    'BLEU-1': [],\n",
    "    'BLEU-2': [],\n",
    "    'BLEU-3': [],\n",
    "    'BLEU-4': [],\n",
    "    'METEOR': [],\n",
    "    'ROUGE-L': [],\n",
    "    'F1-Score': []\n",
    "}\n",
    "\n",
    "# Filtrer les images qui ont à la fois des références et des prédictions\n",
    "common_images = list(set(references.keys()).intersection(set(generated.keys())))\n",
    "print(f\"Nombre d'images à évaluer: {len(common_images)}\")\n",
    "\n",
    "# Calcul des métriques classiques\n",
    "for img_name in common_images:\n",
    "    refs = references[img_name]\n",
    "    hyp = generated[img_name]\n",
    "    \n",
    "    # BLEU scores\n",
    "    bleu1, bleu2, bleu3, bleu4 = calculate_bleu_scores(refs, hyp)\n",
    "    metrics['BLEU-1'].append(bleu1)\n",
    "    metrics['BLEU-2'].append(bleu2)\n",
    "    metrics['BLEU-3'].append(bleu3)\n",
    "    metrics['BLEU-4'].append(bleu4)\n",
    "    \n",
    "    # METEOR\n",
    "    meteor = calculate_meteor(refs, hyp)\n",
    "    metrics['METEOR'].append(meteor)\n",
    "    \n",
    "    # ROUGE-L\n",
    "    rouge_l = calculate_rouge_l(refs, hyp)\n",
    "    metrics['ROUGE-L'].append(rouge_l)\n",
    "    \n",
    "    # F1-Score\n",
    "    f1 = calculate_f1_score(refs, hyp)\n",
    "    metrics['F1-Score'].append(f1)\n",
    "\n",
    "# Calcul CIDEr\n",
    "if CIDER_AVAILABLE:\n",
    "    print(\"\\nCalcul de CIDEr...\")\n",
    "    try:\n",
    "        gts, res = prepare_for_coco_eval(references, generated, common_images)\n",
    "        cider_scorer = Cider()\n",
    "        cider_score, _ = cider_scorer.compute_score(gts, res)\n",
    "        metrics['CIDEr'] = [cider_score]\n",
    "        print(f\"✅ CIDEr: {cider_score:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur CIDEr: {e}\")\n",
    "        CIDER_AVAILABLE = False\n",
    "\n",
    "# Calcul SPICE\n",
    "if SPICE_AVAILABLE:\n",
    "    print(\"\\nCalcul de SPICE (peut prendre du temps)...\")\n",
    "    try:\n",
    "        gts, res = prepare_for_coco_eval(references, generated, common_images)\n",
    "        spice_scorer = Spice()\n",
    "        spice_score, _ = spice_scorer.compute_score(gts, res)\n",
    "        metrics['SPICE'] = [spice_score]\n",
    "        print(f\"✅ SPICE: {spice_score:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur SPICE: {e}\")\n",
    "        print(\"Note: SPICE nécessite Java.\")\n",
    "        SPICE_AVAILABLE = False\n",
    "\n",
    "# ==================== CALCUL DES MOYENNES ====================\n",
    "results = {\n",
    "    'model_name': nom_model,\n",
    "    'num_images_evaluated': len(common_images),\n",
    "    'metrics': {}\n",
    "}\n",
    "\n",
    "for metric_name, values in metrics.items():\n",
    "    if len(values) > 0:\n",
    "        results['metrics'][metric_name] = {\n",
    "            'mean': float(np.mean(values)),\n",
    "            'std': float(np.std(values)) if len(values) > 1 else 0.0,\n",
    "            'min': float(np.min(values)),\n",
    "            'max': float(np.max(values))\n",
    "        }\n",
    "\n",
    "# ==================== SAUVEGARDE DES RÉSULTATS ====================\n",
    "print(f\"\\nSauvegarde des résultats dans {output_json}...\")\n",
    "\n",
    "with open(output_json, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, indent=4, fp=f)\n",
    "\n",
    "# ==================== AFFICHAGE DES RÉSULTATS ====================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"RÉSULTATS D'ÉVALUATION - {nom_model}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Nombre d'images évaluées: {len(common_images)}\")\n",
    "print(\"\\nMétriques moyennes:\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "metric_order = ['BLEU-1', 'BLEU-2', 'BLEU-3', 'BLEU-4', 'METEOR', 'ROUGE-L', 'CIDEr', 'SPICE', 'F1-Score']\n",
    "\n",
    "for metric_name in metric_order:\n",
    "    if metric_name in results['metrics']:\n",
    "        mean_val = results['metrics'][metric_name]['mean']\n",
    "        std_val = results['metrics'][metric_name]['std']\n",
    "        print(f\"{metric_name:12s}: {mean_val:.4f} ± {std_val:.4f}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n✅ Résultats sauvegardés dans: {output_json}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3022ded2-55fe-439c-aca4-56f1eebc5ecf",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fa761f8-7ac6-48a5-aa93-58be4e46e4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TESTS DE VÉRIFICATION\n",
      "============================================================\n",
      "\n",
      "✓ Test 1 - Chargement des données\n",
      "  - Références : 4050 lignes\n",
      "  - Images uniques : 810\n",
      "  - Captions générées : 810\n",
      "  - Images en commun : 810\n",
      "\n",
      "✓ Test 2 - Exemples de données\n",
      "\n",
      "Image : 3041487045_b48ac7ed08.jpg\n",
      "Références (5) :\n",
      "  1. A dog runs through the woods .\n",
      "  2. A pale tan dog romps through a wooded area\n",
      "  3. A tan dog is carrying a small object in its mouth .\n",
      "  4. a tan dog running\n",
      "  5. A yellow dog is trotting through the leaves .\n",
      "\n",
      "Générée :\n",
      "  a brown dog is running on a dirt path .\n",
      "\n",
      "✓ Test 3 - Test BLEU sur l'exemple\n",
      "  BLEU-1: 0.6000\n",
      "  BLEU-2: 0.2582\n",
      "  BLEU-3: 0.0964\n",
      "  BLEU-4: 0.0587\n",
      "\n",
      "✓ Test 4 - Vérification du fichier JSON\n",
      "  ✅ Fichier créé : evaluations/evaluation_model_ft_vgg_dense_transformer.json\n",
      "  ✅ Nombre de métriques sauvegardées : 9\n",
      "\n",
      "============================================================\n",
      "TOUS LES TESTS TERMINÉS ✅\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# === CELLULE DE TEST ===\n",
    "print(\"=\"*60)\n",
    "print(\"TESTS DE VÉRIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test 1 : Vérifier le chargement\n",
    "print(f\"\\n✓ Test 1 - Chargement des données\")\n",
    "print(f\"  - Références : {len(test_df)} lignes\")\n",
    "print(f\"  - Images uniques : {len(references)}\")\n",
    "print(f\"  - Captions générées : {len(generated)}\")\n",
    "print(f\"  - Images en commun : {len(common_images)}\")\n",
    "\n",
    "# Test 2 : Afficher quelques exemples\n",
    "print(f\"\\n✓ Test 2 - Exemples de données\")\n",
    "sample_img = list(common_images)[0]\n",
    "print(f\"\\nImage : {sample_img}\")\n",
    "print(f\"Références ({len(references[sample_img])}) :\")\n",
    "for i, ref in enumerate(references[sample_img], 1):\n",
    "    print(f\"  {i}. {ref}\")\n",
    "print(f\"\\nGénérée :\")\n",
    "print(f\"  {generated[sample_img]}\")\n",
    "\n",
    "# Test 3 : Tester une métrique sur un exemple\n",
    "print(f\"\\n✓ Test 3 - Test BLEU sur l'exemple\")\n",
    "refs = references[sample_img]\n",
    "hyp = generated[sample_img]\n",
    "bleu1, bleu2, bleu3, bleu4 = calculate_bleu_scores(refs, hyp)\n",
    "print(f\"  BLEU-1: {bleu1:.4f}\")\n",
    "print(f\"  BLEU-2: {bleu2:.4f}\")\n",
    "print(f\"  BLEU-3: {bleu3:.4f}\")\n",
    "print(f\"  BLEU-4: {bleu4:.4f}\")\n",
    "\n",
    "# Test 4 : Vérifier que le JSON est bien créé\n",
    "print(f\"\\n✓ Test 4 - Vérification du fichier JSON\")\n",
    "import os\n",
    "if os.path.exists(output_json):\n",
    "    print(f\"  ✅ Fichier créé : {output_json}\")\n",
    "    with open(output_json, 'r') as f:\n",
    "        saved_results = json.load(f)\n",
    "    print(f\"  ✅ Nombre de métriques sauvegardées : {len(saved_results['metrics'])}\")\n",
    "else:\n",
    "    print(f\"  ❌ Fichier non trouvé\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TOUS LES TESTS TERMINÉS ✅\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59e4522-047d-47aa-adca-447caee5db3f",
   "metadata": {},
   "source": [
    "schema du fichier des captions: \n",
    "\n",
    "\n",
    "1022454428_b6b660a67b.jpg|a man in a black shirt and a woman in a white shirt are sitting on a bench .\n",
    "\n",
    "102351840_323e3de834.jpg|a man is standing on a red board with a man in the background .\n",
    "\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab7a3a3-32a2-4e29-a6e9-fd4b46589e02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Rachid CV)",
   "language": "python",
   "name": "rbenhsina_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
