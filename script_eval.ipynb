{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0911ef4-d502-4c98-9536-73b94f4798cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CIDEr et SPICE disponibles\n",
      "✅ BERTScore disponible\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from rouge_score import rouge_scorer\n",
    "import nltk\n",
    "\n",
    "# Télécharger les ressources NLTK nécessaires (à faire une seule fois)\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "except LookupError:\n",
    "    print(\"Téléchargement des ressources NLTK...\")\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('punkt_tab')\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4')\n",
    "    print(\"✅ Ressources NLTK téléchargées!\")\n",
    "\n",
    "# ==================== IMPORT PYCOCOEVALCAP ====================\n",
    "try:\n",
    "    from pycocoevalcap.cider.cider import Cider\n",
    "    from pycocoevalcap.spice.spice import Spice\n",
    "    CIDER_AVAILABLE = True\n",
    "    SPICE_AVAILABLE = True\n",
    "    print(\"✅ CIDEr et SPICE disponibles\")\n",
    "except ImportError:\n",
    "    print(\"⚠️  pycocoevalcap non installé.\")\n",
    "    CIDER_AVAILABLE = False\n",
    "    SPICE_AVAILABLE = False\n",
    "\n",
    "# ==================== IMPORT BERTSCORE ====================\n",
    "try:\n",
    "    from bert_score import score as bert_score_fn\n",
    "    BERTSCORE_AVAILABLE = True\n",
    "    print(\"✅ BERTScore disponible\")\n",
    "except ImportError:\n",
    "    print(\"⚠️  BERTScore non installé. Installation en cours...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"bert-score\"])\n",
    "    try:\n",
    "        from bert_score import score as bert_score_fn\n",
    "        BERTSCORE_AVAILABLE = True\n",
    "        print(\"✅ BERTScore installé!\")\n",
    "    except ImportError:\n",
    "        print(\"❌ Impossible d'installer BERTScore\")\n",
    "        BERTSCORE_AVAILABLE = False\n",
    "\n",
    "# ==================== FONCTIONS D'ÉVALUATION ====================\n",
    "def tokenize(text):\n",
    "    \"\"\"Tokenize le texte en mots\"\"\"\n",
    "    return nltk.word_tokenize(text.lower())\n",
    "\n",
    "def calculate_bleu_scores(references_list, hypothesis):\n",
    "    \"\"\"Calcule BLEU-1 à BLEU-4\"\"\"\n",
    "    smooth = SmoothingFunction()\n",
    "    \n",
    "    # Tokenizer\n",
    "    ref_tokens = [tokenize(ref) for ref in references_list]\n",
    "    hyp_tokens = tokenize(hypothesis)\n",
    "    \n",
    "    bleu1 = sentence_bleu(ref_tokens, hyp_tokens, weights=(1, 0, 0, 0), \n",
    "                          smoothing_function=smooth.method1)\n",
    "    bleu2 = sentence_bleu(ref_tokens, hyp_tokens, weights=(0.5, 0.5, 0, 0), \n",
    "                          smoothing_function=smooth.method1)\n",
    "    bleu3 = sentence_bleu(ref_tokens, hyp_tokens, weights=(0.33, 0.33, 0.33, 0), \n",
    "                          smoothing_function=smooth.method1)\n",
    "    bleu4 = sentence_bleu(ref_tokens, hyp_tokens, weights=(0.25, 0.25, 0.25, 0.25), \n",
    "                          smoothing_function=smooth.method1)\n",
    "    \n",
    "    return bleu1, bleu2, bleu3, bleu4\n",
    "\n",
    "def calculate_meteor(references_list, hypothesis):\n",
    "    \"\"\"Calcule METEOR score\"\"\"\n",
    "    ref_tokens = [tokenize(ref) for ref in references_list]\n",
    "    hyp_tokens = tokenize(hypothesis)\n",
    "    \n",
    "    # METEOR nécessite une seule référence à la fois, on prend la moyenne\n",
    "    scores = [meteor_score([ref], hyp_tokens) for ref in ref_tokens]\n",
    "    return np.mean(scores)\n",
    "\n",
    "def calculate_rouge_l(references_list, hypothesis):\n",
    "    \"\"\"Calcule ROUGE-L score\"\"\"\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    \n",
    "    scores = []\n",
    "    for ref in references_list:\n",
    "        score = scorer.score(ref, hypothesis)\n",
    "        scores.append(score['rougeL'].fmeasure)\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "def calculate_f1_score(references_list, hypothesis):\n",
    "    \"\"\"Calcule F1-score basé sur les mots communs\"\"\"\n",
    "    hyp_words = set(tokenize(hypothesis))\n",
    "    \n",
    "    f1_scores = []\n",
    "    for ref in references_list:\n",
    "        ref_words = set(tokenize(ref))\n",
    "        \n",
    "        if len(hyp_words) == 0 and len(ref_words) == 0:\n",
    "            f1_scores.append(1.0)\n",
    "            continue\n",
    "        \n",
    "        common = hyp_words.intersection(ref_words)\n",
    "        \n",
    "        if len(common) == 0:\n",
    "            f1_scores.append(0.0)\n",
    "            continue\n",
    "        \n",
    "        precision = len(common) / len(hyp_words) if len(hyp_words) > 0 else 0\n",
    "        recall = len(common) / len(ref_words) if len(ref_words) > 0 else 0\n",
    "        \n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    return np.mean(f1_scores)\n",
    "\n",
    "def prepare_for_coco_eval(references_dict, generated_dict, common_images):\n",
    "    \"\"\"Prépare les données au format attendu par pycocoevalcap\"\"\"\n",
    "    gts = {}\n",
    "    res = {}\n",
    "    \n",
    "    for idx, img_name in enumerate(common_images):\n",
    "        # Format: liste de strings directement\n",
    "        gts[idx] = references_dict[img_name]\n",
    "        res[idx] = [generated_dict[img_name]]\n",
    "    \n",
    "    return gts, res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eeb63e0c-cacc-4a8e-bf47-0f4a11ec2e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== CONFIGURATION ====================\n",
    "test_captions_path = \"outputs/test_captions.csv\"\n",
    "generated_captions_path = \"outputs/generated_captions/generated_captions_ft_vgg_dense_transformer.csv\"\n",
    "nom_model = \"ft_vgg_dense_transformer\"\n",
    "output_json = f\"evaluations/evaluation_model_{nom_model}.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc36af15-c62a-4caa-be26-a6d4f6e7ac74",
   "metadata": {},
   "source": [
    "Exemple du schema du fichier des captions: \n",
    "\n",
    "\n",
    "1022454428_b6b660a67b.jpg|a man in a black shirt and a woman in a white shirt are sitting on a bench .\n",
    "\n",
    "102351840_323e3de834.jpg|a man is standing on a red board with a man in the background .\n",
    "\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "325c6685-3430-4c24-8e2f-1cb88da6b806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des données...\n",
      "Captions de référence chargées: 4050 lignes\n",
      "Captions générées chargées: 810 lignes\n",
      "Nombre d'images uniques: 810\n",
      "Nombre de captions générées: 810\n",
      "\n",
      "Calcul des métriques...\n",
      "Nombre d'images à évaluer: 810\n",
      "\n",
      "Calcul de CIDEr...\n",
      "✅ CIDEr: 0.4929\n",
      "\n",
      "Calcul de SPICE (peut prendre du temps)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.nustaq.serialization.FSTClazzInfo (file:/data1/rbenhsina/rbenhsina_venv/lib/python3.10/site-packages/pycocoevalcap/spice/lib/fst-2.47.jar) to field java.lang.String.value\n",
      "WARNING: Please consider reporting this to the maintainers of org.nustaq.serialization.FSTClazzInfo\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Warning: Nashorn engine is planned to be removed from a future JDK release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 968.4 ms\n",
      "✅ SPICE: 0.1391\n",
      "\n",
      "Calcul de BERTScore (peut prendre du temps)...\n",
      "  Calcul en cours (utilise BERT, peut être long)...\n",
      "✅ BERTScore: 0.5664\n",
      "\n",
      "Sauvegarde des résultats dans evaluations/evaluation_model_ft_vgg_dense_transformer.json...\n",
      "\n",
      "============================================================\n",
      "RÉSULTATS D'ÉVALUATION - ft_vgg_dense_transformer\n",
      "============================================================\n",
      "Nombre d'images évaluées: 810\n",
      "\n",
      "Métriques moyennes:\n",
      "------------------------------------------------------------\n",
      "BLEU-1      : 0.6145 ± 0.1829\n",
      "BLEU-2      : 0.4039 ± 0.2264\n",
      "BLEU-3      : 0.2683 ± 0.2173\n",
      "BLEU-4      : 0.1779 ± 0.1822\n",
      "METEOR      : 0.2836 ± 0.1292\n",
      "ROUGE-L     : 0.3196 ± 0.1275\n",
      "CIDEr       : 0.4929 ± 0.0000\n",
      "SPICE       : 0.1391 ± 0.0000\n",
      "BERTScore   : 0.5664 ± 0.1417\n",
      "F1-Score    : 0.3701 ± 0.1181\n",
      "============================================================\n",
      "\n",
      "✅ Résultats sauvegardés dans: evaluations/evaluation_model_ft_vgg_dense_transformer.json\n"
     ]
    }
   ],
   "source": [
    "# ==================== CHARGEMENT DES DONNÉES ====================\n",
    "print(\"Chargement des données...\")\n",
    "\n",
    "# Charger les captions de référence (AVEC skiprows=1 pour ignorer le header)\n",
    "test_df = pd.read_csv(test_captions_path, engine='python', sep='|', \n",
    "                      names=['image', 'caption'], skiprows=1)\n",
    "print(f\"Captions de référence chargées: {len(test_df)} lignes\")\n",
    "\n",
    "# Charger les captions générées\n",
    "gen_df = pd.read_csv(generated_captions_path, engine='python', sep='|', \n",
    "                     names=['image', 'caption'])\n",
    "print(f\"Captions générées chargées: {len(gen_df)} lignes\")\n",
    "\n",
    "# Organiser les captions de référence par image\n",
    "references = defaultdict(list)\n",
    "for _, row in test_df.iterrows():\n",
    "    img_name = row['image'].strip()\n",
    "    caption = row['caption'].strip()\n",
    "    references[img_name].append(caption)\n",
    "\n",
    "print(f\"Nombre d'images uniques: {len(references)}\")\n",
    "\n",
    "# Organiser les captions générées\n",
    "generated = {}\n",
    "for _, row in gen_df.iterrows():\n",
    "    img_name = row['image'].strip()\n",
    "    caption = row['caption'].strip()\n",
    "    generated[img_name] = caption\n",
    "\n",
    "print(f\"Nombre de captions générées: {len(generated)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ==================== CALCUL DES MÉTRIQUES ====================\n",
    "print(\"\\nCalcul des métriques...\")\n",
    "\n",
    "metrics = {\n",
    "    'BLEU-1': [],\n",
    "    'BLEU-2': [],\n",
    "    'BLEU-3': [],\n",
    "    'BLEU-4': [],\n",
    "    'METEOR': [],\n",
    "    'ROUGE-L': [],\n",
    "    'F1-Score': []\n",
    "}\n",
    "\n",
    "# Filtrer les images qui ont à la fois des références et des prédictions\n",
    "common_images = list(set(references.keys()).intersection(set(generated.keys())))\n",
    "print(f\"Nombre d'images à évaluer: {len(common_images)}\")\n",
    "\n",
    "# Calcul des métriques classiques\n",
    "for img_name in common_images:\n",
    "    refs = references[img_name]\n",
    "    hyp = generated[img_name]\n",
    "    \n",
    "    # BLEU scores\n",
    "    bleu1, bleu2, bleu3, bleu4 = calculate_bleu_scores(refs, hyp)\n",
    "    metrics['BLEU-1'].append(bleu1)\n",
    "    metrics['BLEU-2'].append(bleu2)\n",
    "    metrics['BLEU-3'].append(bleu3)\n",
    "    metrics['BLEU-4'].append(bleu4)\n",
    "    \n",
    "    # METEOR\n",
    "    meteor = calculate_meteor(refs, hyp)\n",
    "    metrics['METEOR'].append(meteor)\n",
    "    \n",
    "    # ROUGE-L\n",
    "    rouge_l = calculate_rouge_l(refs, hyp)\n",
    "    metrics['ROUGE-L'].append(rouge_l)\n",
    "    \n",
    "    # F1-Score\n",
    "    f1 = calculate_f1_score(refs, hyp)\n",
    "    metrics['F1-Score'].append(f1)\n",
    "\n",
    "# Calcul CIDEr\n",
    "if CIDER_AVAILABLE:\n",
    "    print(\"\\nCalcul de CIDEr...\")\n",
    "    try:\n",
    "        gts, res = prepare_for_coco_eval(references, generated, common_images)\n",
    "        cider_scorer = Cider()\n",
    "        cider_score, _ = cider_scorer.compute_score(gts, res)\n",
    "        metrics['CIDEr'] = [cider_score]\n",
    "        print(f\"✅ CIDEr: {cider_score:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur CIDEr: {e}\")\n",
    "        CIDER_AVAILABLE = False\n",
    "\n",
    "# Calcul SPICE\n",
    "if SPICE_AVAILABLE:\n",
    "    print(\"\\nCalcul de SPICE (peut prendre du temps)...\")\n",
    "    try:\n",
    "        gts, res = prepare_for_coco_eval(references, generated, common_images)\n",
    "        spice_scorer = Spice()\n",
    "        spice_score, _ = spice_scorer.compute_score(gts, res)\n",
    "        metrics['SPICE'] = [spice_score]\n",
    "        print(f\"✅ SPICE: {spice_score:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur SPICE: {e}\")\n",
    "        print(\"Note: SPICE nécessite Java.\")\n",
    "        SPICE_AVAILABLE = False\n",
    "\n",
    "# Calcul BERTScore\n",
    "if BERTSCORE_AVAILABLE:\n",
    "    print(\"\\nCalcul de BERTScore (peut prendre du temps)...\")\n",
    "    try:\n",
    "        # Préparer les listes\n",
    "        all_candidates = []\n",
    "        all_references_flat = []\n",
    "        \n",
    "        for img_name in common_images:\n",
    "            candidate = generated[img_name]\n",
    "            refs = references[img_name]\n",
    "            \n",
    "            # Pour chaque candidate, on duplique pour comparer avec toutes les références\n",
    "            all_candidates.extend([candidate] * len(refs))\n",
    "            all_references_flat.extend(refs)\n",
    "        \n",
    "        # Calculer BERTScore pour toutes les paires\n",
    "        print(\"  Calcul en cours (utilise BERT, peut être long)...\")\n",
    "        P, R, F1 = bert_score_fn(\n",
    "            all_candidates, \n",
    "            all_references_flat, \n",
    "            lang='en',\n",
    "            model_type='bert-base-uncased',\n",
    "            rescale_with_baseline=True,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Réorganiser les scores par image (prendre le max F1 parmi les 5 références)\n",
    "        F1_list = []\n",
    "        idx = 0\n",
    "        \n",
    "        for img_name in common_images:\n",
    "            num_refs = len(references[img_name])\n",
    "            \n",
    "            # Extraire les scores F1 pour cette image\n",
    "            f1_scores = F1[idx:idx+num_refs]\n",
    "            \n",
    "            # Prendre le meilleur F1\n",
    "            best_f1 = f1_scores.max().item()\n",
    "            F1_list.append(best_f1)\n",
    "            \n",
    "            idx += num_refs\n",
    "        \n",
    "        # Stocker dans metrics\n",
    "        metrics['BERTScore'] = F1_list\n",
    "        \n",
    "        print(f\"✅ BERTScore: {np.mean(F1_list):.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur BERTScore: {e}\")\n",
    "        BERTSCORE_AVAILABLE = False\n",
    "\n",
    "# ==================== CALCUL DES MOYENNES ====================\n",
    "results = {\n",
    "    'model_name': nom_model,\n",
    "    'num_images_evaluated': len(common_images),\n",
    "    'metrics': {}\n",
    "}\n",
    "\n",
    "for metric_name, values in metrics.items():\n",
    "    if len(values) > 0:\n",
    "        results['metrics'][metric_name] = {\n",
    "            'mean': float(np.mean(values)),\n",
    "            'std': float(np.std(values)) if len(values) > 1 else 0.0,\n",
    "            'min': float(np.min(values)),\n",
    "            'max': float(np.max(values))\n",
    "        }\n",
    "\n",
    "# ==================== SAUVEGARDE DES RÉSULTATS ====================\n",
    "print(f\"\\nSauvegarde des résultats dans {output_json}...\")\n",
    "\n",
    "with open(output_json, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, indent=4, fp=f)\n",
    "\n",
    "# ==================== AFFICHAGE DES RÉSULTATS ====================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"RÉSULTATS D'ÉVALUATION - {nom_model}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Nombre d'images évaluées: {len(common_images)}\")\n",
    "print(\"\\nMétriques moyennes:\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "metric_order = ['BLEU-1', 'BLEU-2', 'BLEU-3', 'BLEU-4', 'METEOR', 'ROUGE-L', \n",
    "                'CIDEr', 'SPICE', 'BERTScore', 'F1-Score']\n",
    "\n",
    "for metric_name in metric_order:\n",
    "    if metric_name in results['metrics']:\n",
    "        mean_val = results['metrics'][metric_name]['mean']\n",
    "        std_val = results['metrics'][metric_name]['std']\n",
    "        print(f\"{metric_name:12s}: {mean_val:.4f} ± {std_val:.4f}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n✅ Résultats sauvegardés dans: {output_json}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3022ded2-55fe-439c-aca4-56f1eebc5ecf",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fa761f8-7ac6-48a5-aa93-58be4e46e4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TESTS DE VÉRIFICATION\n",
      "============================================================\n",
      "\n",
      "✓ Test 1 - Chargement des données\n",
      "  - Références : 4050 lignes\n",
      "  - Images uniques : 810\n",
      "  - Captions générées : 810\n",
      "  - Images en commun : 810\n",
      "\n",
      "✓ Test 2 - Exemples de données\n",
      "\n",
      "Image : 3041487045_b48ac7ed08.jpg\n",
      "Références (5) :\n",
      "  1. A dog runs through the woods .\n",
      "  2. A pale tan dog romps through a wooded area\n",
      "  3. A tan dog is carrying a small object in its mouth .\n",
      "  4. a tan dog running\n",
      "  5. A yellow dog is trotting through the leaves .\n",
      "\n",
      "Générée :\n",
      "  a brown dog is running on a dirt path .\n",
      "\n",
      "✓ Test 3 - Test des métriques sur l'exemple\n",
      "  BLEU-1: 0.6000\n",
      "  BLEU-2: 0.2582\n",
      "  BLEU-3: 0.0964\n",
      "  BLEU-4: 0.0587\n",
      "  METEOR: 0.2854\n",
      "  ROUGE-L: 0.3896\n",
      "  F1-Score: 0.3833\n",
      "  BERTScore: 0.6191\n",
      "\n",
      "✓ Test 4 - Vérification du fichier JSON\n",
      "  ✅ Fichier créé : evaluations/evaluation_model_ft_vgg_dense_transformer.json\n",
      "  ✅ Nombre de métriques sauvegardées : 10\n",
      "\n",
      "  Métriques disponibles dans le JSON :\n",
      "    - BLEU-1: 0.6145\n",
      "    - BLEU-2: 0.4039\n",
      "    - BLEU-3: 0.2683\n",
      "    - BLEU-4: 0.1779\n",
      "    - METEOR: 0.2836\n",
      "    - ROUGE-L: 0.3196\n",
      "    - F1-Score: 0.3701\n",
      "    - CIDEr: 0.4929\n",
      "    - SPICE: 0.1391\n",
      "    - BERTScore: 0.5664\n",
      "\n",
      "✓ Test 5 - Statistiques globales\n",
      "  - Taux de couverture : 100.0%\n",
      "  - Images manquantes : 0\n",
      "\n",
      "✓ Test 6 - Cohérence des métriques\n",
      "  ✅ Toutes les métriques ont le bon nombre de valeurs\n",
      "\n",
      "============================================================\n",
      "TOUS LES TESTS TERMINÉS ✅\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# === CELLULE DE TEST ===\n",
    "import os\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TESTS DE VÉRIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test 1 : Vérifier le chargement\n",
    "print(f\"\\n✓ Test 1 - Chargement des données\")\n",
    "print(f\"  - Références : {len(test_df)} lignes\")\n",
    "print(f\"  - Images uniques : {len(references)}\")\n",
    "print(f\"  - Captions générées : {len(generated)}\")\n",
    "print(f\"  - Images en commun : {len(common_images)}\")\n",
    "\n",
    "# Test 2 : Afficher quelques exemples\n",
    "print(f\"\\n✓ Test 2 - Exemples de données\")\n",
    "sample_img = list(common_images)[0]\n",
    "print(f\"\\nImage : {sample_img}\")\n",
    "print(f\"Références ({len(references[sample_img])}) :\")\n",
    "for i, ref in enumerate(references[sample_img], 1):\n",
    "    print(f\"  {i}. {ref}\")\n",
    "print(f\"\\nGénérée :\")\n",
    "print(f\"  {generated[sample_img]}\")\n",
    "\n",
    "# Test 3 : Tester les métriques sur un exemple\n",
    "print(f\"\\n✓ Test 3 - Test des métriques sur l'exemple\")\n",
    "refs = references[sample_img]\n",
    "hyp = generated[sample_img]\n",
    "\n",
    "# BLEU\n",
    "bleu1, bleu2, bleu3, bleu4 = calculate_bleu_scores(refs, hyp)\n",
    "print(f\"  BLEU-1: {bleu1:.4f}\")\n",
    "print(f\"  BLEU-2: {bleu2:.4f}\")\n",
    "print(f\"  BLEU-3: {bleu3:.4f}\")\n",
    "print(f\"  BLEU-4: {bleu4:.4f}\")\n",
    "\n",
    "# METEOR\n",
    "meteor = calculate_meteor(refs, hyp)\n",
    "print(f\"  METEOR: {meteor:.4f}\")\n",
    "\n",
    "# ROUGE-L\n",
    "rouge_l = calculate_rouge_l(refs, hyp)\n",
    "print(f\"  ROUGE-L: {rouge_l:.4f}\")\n",
    "\n",
    "# F1-Score\n",
    "f1 = calculate_f1_score(refs, hyp)\n",
    "print(f\"  F1-Score: {f1:.4f}\")\n",
    "\n",
    "# BERTScore (si disponible)\n",
    "if BERTSCORE_AVAILABLE:\n",
    "    try:\n",
    "        P, R, F1 = bert_score_fn([hyp] * len(refs), refs, lang='en', \n",
    "                                  model_type='bert-base-uncased', \n",
    "                                  rescale_with_baseline=True, verbose=False)\n",
    "        best_f1 = F1.max().item()\n",
    "        print(f\"  BERTScore: {best_f1:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  BERTScore: ❌ Erreur - {e}\")\n",
    "else:\n",
    "    print(f\"  BERTScore: ⚠️ Non disponible\")\n",
    "\n",
    "# Test 4 : Vérifier que le JSON est bien créé\n",
    "print(f\"\\n✓ Test 4 - Vérification du fichier JSON\")\n",
    "if os.path.exists(output_json):\n",
    "    print(f\"  ✅ Fichier créé : {output_json}\")\n",
    "    with open(output_json, 'r') as f:\n",
    "        saved_results = json.load(f)\n",
    "    print(f\"  ✅ Nombre de métriques sauvegardées : {len(saved_results['metrics'])}\")\n",
    "    \n",
    "    # Afficher toutes les métriques disponibles\n",
    "    print(f\"\\n  Métriques disponibles dans le JSON :\")\n",
    "    for metric_name in saved_results['metrics'].keys():\n",
    "        mean_val = saved_results['metrics'][metric_name]['mean']\n",
    "        print(f\"    - {metric_name}: {mean_val:.4f}\")\n",
    "else:\n",
    "    print(f\"  ❌ Fichier non trouvé\")\n",
    "\n",
    "# Test 5 : Statistiques globales\n",
    "print(f\"\\n✓ Test 5 - Statistiques globales\")\n",
    "print(f\"  - Taux de couverture : {len(common_images)/len(references)*100:.1f}%\")\n",
    "print(f\"  - Images manquantes : {len(references) - len(common_images)}\")\n",
    "\n",
    "if len(common_images) < len(references):\n",
    "    missing_images = set(references.keys()) - set(generated.keys())\n",
    "    print(f\"  - Exemples d'images manquantes :\")\n",
    "    for i, img in enumerate(list(missing_images)[:3], 1):\n",
    "        print(f\"    {i}. {img}\")\n",
    "\n",
    "# Test 6 : Vérifier la cohérence des métriques\n",
    "print(f\"\\n✓ Test 6 - Cohérence des métriques\")\n",
    "metric_counts = {name: len(values) for name, values in metrics.items()}\n",
    "expected_count = len(common_images)\n",
    "\n",
    "all_consistent = True\n",
    "for metric_name, count in metric_counts.items():\n",
    "    if metric_name not in ['CIDEr', 'SPICE', 'BERTScore']:  # Ces métriques sont globales\n",
    "        if count != expected_count:\n",
    "            print(f\"  ❌ {metric_name}: {count} valeurs (attendu: {expected_count})\")\n",
    "            all_consistent = False\n",
    "\n",
    "if all_consistent:\n",
    "    print(f\"  ✅ Toutes les métriques ont le bon nombre de valeurs\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TOUS LES TESTS TERMINÉS ✅\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59e4522-047d-47aa-adca-447caee5db3f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab7a3a3-32a2-4e29-a6e9-fd4b46589e02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Rachid CV)",
   "language": "python",
   "name": "rbenhsina_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
